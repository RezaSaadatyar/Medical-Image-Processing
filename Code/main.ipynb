{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Welcome to Medical image processing in Python***<br/>\n",
    "\n",
    "Presented by: Reza Saadatyar (2024-2025) <br/>\n",
    "E-mail: Reza.Saadatyar@outlook.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1️⃣ Set Image Path & Load data**<br/>\n",
    "\n",
    "**2️⃣ Convert the images into an array & masks to boolean**<br/>\n",
    "\n",
    "**3️⃣ RGB to gray**<br/>\n",
    "\n",
    "**4️⃣ [Image Resizing](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize)**<br/>\n",
    "`Standardizing Image Dimensions:` Machine learning models, like CNNs, need input data with fixed dimensions. For instance, if a model requires images of size 224x224x3, all input images must be resized to that shape.<br/>\n",
    "`Reducing Computational Load:`Resizing images to smaller dimensions lowers computational costs, particularly with large datasets, and aids in faster training or inference for deep learning models.\n",
    "\n",
    "**5️⃣ Augmentation**<br/>\n",
    "*Purpose:*<br/>\n",
    "- `Increase Dataset Size:` Augmentation creates new training samples from existing ones, effectively increasing the dataset size.<br/>\n",
    "- `Improve Model Robustness:` Introducing variations such as rotations, flips, and zooms helps the model adapt more effectively to real-world scenarios.<br/>\n",
    "- `Prevent Overfitting:` Augmentation enhances variability, minimizing the likelihood of the model overfitting to the training data.<br/>\n",
    "\n",
    "*Augmentation Techniques:*<br/>\n",
    "- `Rotation:` Rotating images by a specified degree range (e.g., rotation_range=30).<br/>\n",
    "- `Flip:` Flipping images horizontally or vertically.<br/>\n",
    "- `Zoom:` Zooming in or out of images.<br/>\n",
    "- `Shift:` Translating images horizontally or vertically.<br/>\n",
    "- `Brightness/Contrast Adjustment:` Changing the brightness or contrast of images.<br/>\n",
    "- `Noise Addition:` Adding random noise to images.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Image-Width: n*<br/>\n",
    "*Image-Height: m*<br/>\n",
    "*Channels: c*<br/>\n",
    "*Planes: p*<br/>\n",
    "*Grayscale: (p, m, n)*<br/>\n",
    "*RGB: (p, m, n, c)*<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF000e' size=\"4.8\" face=\"Arial\"><b>Import modules</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore\n",
    "# from skimage.viewer import ImageViewer\n",
    "\n",
    "from Functions.filepath_extractor import FilePathExtractor\n",
    "from Functions.image_processor import ImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#070bee size=\"4.5\" face=\"Arial\"><b>1️⃣ Set Image Path & Load data</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mfile_names = ['ytma12_010804_benign2_ccd.tif', 'ytma12_010804_benign3_ccd.tif', 'ytma12_010804_malignant1_ccd.tif', 'ytma12_010804_malignant2_ccd.tif', 'ytma12_010804_malignant3_ccd.tif', 'ytma23_022103_benign1_ccd.tif', 'ytma23_022103_benign2_ccd.tif', 'ytma23_022103_benign3_ccd.tif', 'ytma23_022103_malignant1_ccd.tif', 'ytma23_022103_malignant2_ccd.tif', 'ytma23_022103_malignant3_ccd.tif', 'ytma49_042003_benign1_ccd.tif', 'ytma49_042003_benign2_ccd.tif', 'ytma49_042003_benign3_ccd.tif', 'ytma49_042003_malignant1_ccd.tif', 'ytma49_042003_malignant2_ccd.tif', 'ytma49_042003_malignant3_ccd.tif', 'ytma49_042203_benign1_ccd.tif', 'ytma49_042203_benign2_ccd.tif', 'ytma49_042203_benign3_ccd.tif', 'ytma49_042203_malignant1_ccd.tif', 'ytma49_042203_malignant2_ccd.tif', 'ytma49_042203_malignant3_ccd.tif', 'ytma49_042403_benign1_ccd.tif', 'ytma49_042403_benign2_ccd.tif', 'ytma49_042403_benign3_ccd.tif', 'ytma49_042403_malignant1_ccd.tif', 'ytma49_042403_malignant2_ccd.tif', 'ytma49_042403_malignant3_ccd.tif', 'ytma49_072303_benign1_ccd.tif', 'ytma49_072303_benign2_ccd.tif', 'ytma49_072303_malignant1_ccd.tif', 'ytma49_072303_malignant2_ccd.tif', 'ytma49_111003_benign1_ccd.tif', 'ytma49_111003_benign2_ccd.tif', 'ytma49_111003_benign3_ccd.tif', 'ytma49_111003_malignant1_ccd.tif', 'ytma49_111003_malignant2_ccd.tif', 'ytma49_111003_malignant3_ccd.tif', 'ytma49_111303_benign1_ccd.tif', 'ytma49_111303_benign2_ccd.tif', 'ytma49_111303_benign3_ccd.tif', 'ytma49_111303_malignant1_ccd.tif', 'ytma49_111303_malignant2_ccd.tif', 'ytma49_111303_malignant3_ccd.tif', 'ytma55_030603_benign1_ccd.tif', 'ytma55_030603_benign2_ccd.tif', 'ytma55_030603_benign3_ccd.tif', 'ytma55_030603_benign4_ccd.tif', 'ytma55_030603_benign5_ccd.tif', 'ytma55_030603_benign6_ccd.tif', 'ytma10_010704_benign1_ccd.tif', 'ytma10_010704_benign2_ccd.tif', 'ytma12_010804_benign1_ccd.tif', 'ytma10_010704_malignant2_ccd.tif', 'ytma10_010704_malignant3_ccd.tif', 'ytma10_010704_benign3_ccd.tif', 'ytma10_010704_malignant1_ccd.tif']\n",
      "\u001b[34mfolder = ['D:/Medical-Image-Processing/Data/Inputs', 'D:/Medical-Image-Processing/Data/Inputs/A', 'D:/Medical-Image-Processing/Data/Inputs/A/B', 'D:/Medical-Image-Processing/Data/Inputs/A/B/C']\n",
      "\u001b[35mfiles_path = ['D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign4_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign5_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign6_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/ytma10_010704_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/ytma10_010704_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/ytma12_010804_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/ytma10_010704_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/ytma10_010704_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/C/ytma10_010704_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/C/ytma10_010704_malignant1_ccd.tif']\n",
      "\u001b[36msubfoldersname = ['A', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Inputs\"\n",
    "\n",
    "# Create an instance of DirectoryReader with the directory path and file format\n",
    "obj_path = FilePathExtractor(directory_path, format_type=\"tif\")\n",
    "file_names = obj_path.filesname          # List of filesname in the directory with the specified extension\n",
    "folder = obj_path.folders_path           # List of folders path where the files are located\n",
    "files_path = obj_path.all_files_path     # List of full files path for the files\n",
    "subfoldersname = obj_path.subfoldersname # List of subfolders name within the directory\n",
    "\n",
    "print(Fore.GREEN + f\"{file_names = }\"\"\\n\" + Fore.BLUE + f\"{folder = }\"\"\\n\" + Fore.MAGENTA + f\"{files_path = }\"+\n",
    "      \"\\n\" + Fore.CYAN + f\"{subfoldersname = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#09eb14 size=\"4.5\" face=\"Arial\"><b> 2️⃣ Convert the images into an array</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdata.shape = (58, 768, 896, 3)\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Inputs\"\n",
    "obj = ImageProcessor()\n",
    "data = obj.imgs_to_ndarray(directory_path, format_type=\"tif\")\n",
    "print(Fore.GREEN + f\"{data.shape = }\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#09eb14 size=\"4.5\" face=\"Arial\"><b> 2️⃣ Masks to boolean</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mmasks.shape = (58, 768, 896, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Masks/\"\n",
    "masks = obj.masks_to_boolean(directory_path, format_type=\"TIF\")\n",
    "print(Fore.GREEN + f\"{masks.shape = }\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.subplot(121); plt.imshow(masks[0, :, :, 0], cmap=\"gray\")\n",
    "plt.subplot(122); plt.imshow(masks[0, :, :, 1], cmap=\"gray\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#dd7f05 size=\"4.5\" face=\"Arial\"><b>3️⃣ RGB to Gray</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThe images have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Inputs/\"\n",
    "save_path = 'D:/Medical-Image-Processing/Data/'\n",
    "img_gray = obj.rgb_to_gray(directory_path, save_path, format_type=\"tif\", save_img_gray=\"on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#ec0995 size=\"4.5\" face=\"Arial\"><b>4️⃣ Image Resizing</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mResizing images from (58, 768, 896, 3) to (58, 255, 255, 3)\n"
     ]
    }
   ],
   "source": [
    "# Call the `resize_images` method to resize the images to the target dimensions (255x255)\n",
    "resized_images = obj.resize_images(data, img_height_resized=255, img_width_resized=255)  # Resize all images to 255x255\n",
    "print(Fore.GREEN + f\"Resizing images from {data.shape} to {resized_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#06defa size=\"4.5\" face=\"Arial\"><b>5️⃣ Augmentation</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "rotation_range = 30\n",
    "num_augmented_imag = 3\n",
    "# Path to the folder containing the original images\n",
    "file_path = 'D:/Medical-Image-Processing/Data/Inputs/'\n",
    "\n",
    "# Path where augmented images will be saved\n",
    "augmente_path = 'D:/Medical-Image-Processing/Data/'\n",
    "\n",
    "obj.augmentation(file_path, augmente_path, num_augmented_imag, rotation_range, format_type=\"tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "\n",
    "TRAIN_IMAGE_PATH = 'D:/Medical-Image-Processing/Inputs_Train'\n",
    "TRAIN_MASK_PATH = 'D:/Medical-Image-Processing/Masks_Train/'\n",
    "TEST_IMAGE_PATH = 'D:/Medical-Image-Processing/Inputs_Test/'\n",
    "TEST_MASK_PATH = 'D:/Medical-Image-Processing/Masks_Test/'\n",
    "\n",
    "Train_Mask_List = sorted(next(os.walk(TRAIN_MASK_PATH))[2])\n",
    "Test_Mask_List = sorted(next(os.walk(TEST_MASK_PATH))[2])\n",
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 256, 256, 3\n",
    "Init_Image = np.zeros((len(Train_Mask_List), 768, 896, 3), dtype = np.uint8)\n",
    "Init_Mask = np.zeros((len(Train_Mask_List), 768, 896), dtype = bool)\n",
    "Train_X = np.zeros((len(Train_Mask_List), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype = np.uint8)\n",
    "Train_Y = np.zeros((len(Train_Mask_List), IMG_HEIGHT, IMG_WIDTH, 1), dtype = bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "for mask_path in glob.glob('{}/*.TIF'.format(TRAIN_MASK_PATH)):\n",
    "    \n",
    "    base = os.path.basename(mask_path)\n",
    "    image_ID, ext = os.path.splitext(base)\n",
    "    image_path = '{}/{}_ccd.tif'.format(TRAIN_IMAGE_PATH, image_ID)\n",
    "    mask = io.imread(mask_path)\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    y_coord, x_coord = np.where(mask == 255)\n",
    "    \n",
    "    y_min = min(y_coord) \n",
    "    y_max = max(y_coord)\n",
    "    x_min = min(x_coord)\n",
    "    x_max = max(x_coord)\n",
    "    \n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "    cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    Train_X[n] = transform.resize(cropped_image[:,:,:IMG_CHANNELS],\n",
    "            (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True)\n",
    "    \n",
    "    Train_Y[n] = np.expand_dims(transform.resize(cropped_mask, \n",
    "            (IMG_HEIGHT, IMG_WIDTH),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True), axis = -1)\n",
    "    \n",
    "    Init_Image[n] = image\n",
    "    Init_Mask[n] = mask\n",
    "    \n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X = np.zeros((len(Test_Mask_List), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype = np.uint8)\n",
    "Test_Y = np.zeros((len(Test_Mask_List), IMG_HEIGHT, IMG_WIDTH, 1), dtype = bool)\n",
    "    \n",
    "n = 0\n",
    "\n",
    "for mask_path in glob.glob('{}/*.TIF'.format(TEST_MASK_PATH)):\n",
    "    \n",
    "    base = os.path.basename(mask_path)\n",
    "    image_ID, ext = os.path.splitext(base)\n",
    "    image_path = '{}/{}_ccd.tif'.format(TEST_IMAGE_PATH, image_ID)\n",
    "    mask = io.imread(mask_path)\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    y_coord, x_coord = np.where(mask == 255)\n",
    "    \n",
    "    y_min = min(y_coord) \n",
    "    y_max = max(y_coord)\n",
    "    x_min = min(x_coord)\n",
    "    x_max = max(x_coord)\n",
    "    \n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "    cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    Test_X[n] = transform.resize(cropped_image[:,:,:IMG_CHANNELS],\n",
    "            (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True)\n",
    "    \n",
    "    Test_Y[n] = np.expand_dims(transform.resize(cropped_mask, \n",
    "            (IMG_HEIGHT, IMG_WIDTH),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True), axis = -1)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rows = 1\n",
    "columns = 4\n",
    "Figure = plt.figure(figsize=(15,15))\n",
    "Image_List = [Init_Image[0], Init_Mask[0], Train_X[0], Train_Y[0]]\n",
    "p= ['Original_Image', 'Original_Mask', 'Region_of_Interest_Image', 'Region_of_Interest_Mask']\n",
    "n=0\n",
    "for i in range(1, rows*columns + 1):\n",
    "    Image = Image_List[i-1]\n",
    "    Sub_Plot_Image = Figure.add_subplot(rows, columns, i)\n",
    "    Sub_Plot_Image.imshow(np.squeeze(Image))\n",
    "    plt.title(p[n])\n",
    "    n +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero Padding\n",
    "  - `Preserve Dimensions:` Zero padding retains the original dimensions of the input tensor during convolution, which is important for tasks like image segmentation.\n",
    "  - `Control Output Size:` Zero padding ensures the output tensor size matches desired dimensions, especially in U-Net architectures where input and output sizes must align.\n",
    "  - `Avoid Information Loss:` Pixels near the edges are involved in fewer convolutions than central pixels, leading to potential information loss. Padding ensures every pixel is treated equally.\n",
    "  - `Better Performance:` By maintaining the spatial dimensions, zero padding helps the network extract features effectively, especially for deeper layers.\n",
    "  - Types of Padding\n",
    "    - `Valid Padding:` No padding is applied; the output size decreases.\n",
    "    - `Same Padding:` Zero padding is applied to maintain the same dimensions between input and output.\n",
    "- `Filter`: A filter (or kernel) is a small matrix that slides over input data to perform convolution, extracting features like edges, textures, and patterns (e.g., A 3x3 filter would have 9 weights).\n",
    "- `Stride` is the step size by which the filter or kernel moves across the input during convolution or pooling operations.\n",
    "  - *A stride of 1* moves the filter by one pixel at a time, resulting in a larger output.\n",
    "  - *A stride of 2* skips every alternate pixel, reducing the output size.\n",
    "  - $\\text{Output Size} =  \\large \\frac{\\text{Input Size} - \\text{Filter Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1$\n",
    "    - Input Size (L) = 32\n",
    "    - Filter Size (K) = 3\n",
    "    - Padding (P), (P=0,valid padding)\n",
    "    - Stride (S) = 1\n",
    "    - $\\text{Output Size} = \\frac{32 - 3 + 2 \\times 1}{1} + 1 = \\frac{30 + 2}{1} + 1 = 32$\n",
    "- `Pooling` reduces the spatial dimensions of the feature map, lowering computational complexity and capturing dominant features.\n",
    "   - Max Pooling: 2×2 window, [1, 3; 2, 4] → 4.\n",
    "   - Average Pooling: 2×2 window, [1, 3; 2, 4] → 2.5.\n",
    "- `Upsampling` increases feature map dimensions, used in image segmentation and super-resolution. Methods include Nearest Neighbor Upsampling, which fills new positions with the nearest pixel value, and Bilinear Interpolation, which uses weighted averages for smoother outputs.\n",
    "- `Unpooling` Unpooling reverses pooling to restore feature map resolution, used in image reconstruction or segmentation. Max unpooling replaces max values from pooling in original positions, filling others with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Implementation of U_NET Model for Semantic Segmentation\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "def U_Net_Segmentation(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
    "    \n",
    "    \n",
    "    inputs = layers.Input(input_size)\n",
    "    n = layers.Lambda(lambda x:x/255)(inputs)\n",
    "    \n",
    "    c1 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(n)\n",
    "    c1 = layers.Dropout(0.1)(c1)\n",
    "    c1 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2,2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p1)\n",
    "    c2 = layers.Dropout(0.1)(c2)\n",
    "    c2 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2,2))(c2)\n",
    "\n",
    "\n",
    "    c3 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p2)\n",
    "    c3 = layers.Dropout(0.2)(c3)\n",
    "    c3 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2,2))(c3)\n",
    "\n",
    "\n",
    "    c4 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p3)\n",
    "    c4 = layers.Dropout(0.2)(c4)\n",
    "    c4 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2,2))(c4)\n",
    "\n",
    "\n",
    "    c5 = layers.Conv2D(256, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p4)\n",
    "    c5 = layers.Dropout(0.3)(c5)\n",
    "    c5 = layers.Conv2D(256, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c5)\n",
    "\n",
    "\n",
    "    u6 = layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u6)\n",
    "    c6 = layers.Dropout(0.2)(c6)\n",
    "    c6 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c6)   \n",
    "\n",
    "    u7 = layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u7)\n",
    "    c7 = layers.Dropout(0.2)(c7)\n",
    "    c7 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c7) \n",
    "\n",
    "    u8 = layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u8)\n",
    "    c8 = layers.Dropout(0.1)(c8)\n",
    "    c8 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c8) \n",
    "    \n",
    "    \n",
    "    u9 = layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1], axis = 3)\n",
    "    c9 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u9)\n",
    "    c9 = layers.Dropout(0.1)(c9)\n",
    "    c9 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c9) \n",
    "    \n",
    "    outputs = layers.Conv2D(1,(1,1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=[Mean_IOU_Evaluator])\n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# 6. Define U_NET Model Evaluator (Intersection Over Union _ IOU)\n",
    "# def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "#     prec = []\n",
    "\n",
    "#     # Loop over thresholds from 0.5 to 0.95 (step of 0.05)\n",
    "#     for t in np.arange(0.5, 1, 0.05):\n",
    "#         # Update deprecated functions\n",
    "#         y_pred_ = tf.cast(y_pred > t, tf.int32)  # Replace tf.to_int32 with tf.cast\n",
    "#         metric = tf.keras.metrics.MeanIoU(num_classes=2)  # Updated MeanIoU usage\n",
    "#         metric.update_state(y_true, y_pred_)\n",
    "#         prec.append(metric.result())  # Append IOU result for the threshold\n",
    "\n",
    "#     # Return the mean IOU across all thresholds\n",
    "#     return K.mean(K.stack(prec), axis=0)\n",
    "model = U_Net_Segmentation()\n",
    "\n",
    "def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "    prec = []\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)  # Create the metric instance outside the loop\n",
    "\n",
    "    # Loop over thresholds from 0.5 to 0.95 (step of 0.05)\n",
    "    for t in np.arange(0.5, 1, 0.05):\n",
    "        y_pred_ = tf.cast(y_pred > t, tf.int32)  # Apply threshold\n",
    "        \n",
    "        # Reset the internal variables of MeanIoU\n",
    "        metric.update_state([], [])  # Workaround to clear previous state\n",
    "\n",
    "        metric.update_state(y_true, y_pred_)\n",
    "        prec.append(metric.result().numpy())  # Append IOU result for the threshold\n",
    "\n",
    "    # Return the mean IOU across all thresholds\n",
    "    return K.mean(K.stack(prec), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Define U_NET Model Evaluator (Intersection Over Union _ IOU)\n",
    "# from tensorflow.keras import backend as k\n",
    "# def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "    \n",
    "#     prec = []\n",
    "    \n",
    "#     for t in np.arange(0.5, 1, 0.05):\n",
    "        \n",
    "#         # y_pred_ = tf.to_int32(y_pred>t)\n",
    "#         y_pred_ = tf.cast(y_pred > t, tf.int32)  # Replace tf.to_int32 with tf.cast\n",
    "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "#         k.get_session().run(tf.local_variables_initializer())\n",
    "#         with tf.control_dependencies([up_opt]):\n",
    "#             score = tf.identity(score)\n",
    "#         prec.append(score)\n",
    "#     return k.mean(k.stack(prec), axis = 0)\n",
    "\n",
    "# model = U_Net_Segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageset = 'BCC'\n",
    "backbone = 'UNET'\n",
    "version = 'v1.0'\n",
    "\n",
    "# Correct file path to end with .keras\n",
    "model_keras = 'model-{imageset}-{backbone}-{version}.keras'.format(\n",
    "    imageset=imageset, backbone=backbone, version=version\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "earlystopper = callbacks.EarlyStopping(patience=7, verbose=1)\n",
    "checkpointer = callbacks.ModelCheckpoint(\n",
    "    filepath=model_keras,  # Use the correct file path ending with .keras\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Show The Results per Epoch\n",
    "\n",
    "class loss_history(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__ (self, x=4):\n",
    "        self.x = x\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        \n",
    "        io.imshow(Train_X[self.x])\n",
    "        plt.show()\n",
    "        \n",
    "        io.imshow(np.squeeze(Train_Y[self.x]))\n",
    "        plt.show()\n",
    "        \n",
    "        preds_train = self.model.predict(np.expand_dims(Train_X[self.x], axis = 0))\n",
    "        io.imshow(np.squeeze(preds_train[0]))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(Train_X, Train_Y,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=2,\n",
    "                    epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Show The Results per Epoch\n",
    "\n",
    "class loss_history(callbacks.Callback):\n",
    "\n",
    "    def __init__ (self, x=4):\n",
    "        self.x = x\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \n",
    "        io.imshow(Train_X[self.x])\n",
    "        plt.show()\n",
    "        \n",
    "        io.imshow(np.squeeze(Train_Y[self.x]))\n",
    "        plt.show()\n",
    "        \n",
    "        preds_train = self.model.predict(np.expand_dims(Train_X[self.x], axis = 0))\n",
    "        io.imshow(np.squeeze(preds_train[0]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train U_NET Model using Training Samples\n",
    "\n",
    "results = model.fit(Train_X, Train_Y, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size=2,\n",
    "                    epochs=20,\n",
    "                    callbacks=[earlystopper, checkpointer, loss_history()]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_img_original_augment(self, num_img: int) -> None:\n",
    "        \n",
    "#         _, axs = plt.subplots(nrows=2, ncols=num_img)\n",
    "#          # Check if num_img is 1 (special case for 1 image)\n",
    "#         if num_img == 1:\n",
    "#             # Display images on the first row\n",
    "#             # io.imread(files_inputs[0])[:, :, :3].shape\n",
    "#             axs[0].imshow(io.imread(glob.glob(self.file_path + '/*')[0]), cmap='gray')\n",
    "#             axs[0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "#             [spine.set_visible(False) for spine in axs[0].spines.values()]  # Hide all spines\n",
    "#             axs[0].set_ylabel(\"Original Images\", fontsize=12, labelpad=10)  # Y-axis label for the first row\n",
    "            \n",
    "#             # Display images on the second row\n",
    "#             axs[1].imshow(io.imread(glob.glob(self.ugmente_path + '/*')[0]), cmap='gray')\n",
    "#             axs[1].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "#             [spine.set_visible(False) for spine in axs[1].spines.values()]  # Hide all spines\n",
    "            # axs[1].set_ylabel(\"Augmented Images\", fontsize=12, labelpad=10)  # Y-axis label for the second row\n",
    "\n",
    "    #     else:\n",
    "    #         for i in range(num_img):\n",
    "    #             # Display images on the first row\n",
    "    #             axs[0, i].imshow(io.imread(glob.glob(self.imag_files_path + '/*')[i]), cmap='gray')\n",
    "    #             axs[0, i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "    #             [spine.set_visible(False) for spine in axs[0, i].spines.values()]  # Hide all spines\n",
    "\n",
    "    #             # Display images on the second row\n",
    "    #             axs[1, i].imshow(io.imread(glob.glob(self.imag_augmented_path + '/*')[i]), cmap='gray')\n",
    "    #             axs[1, i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "    #             [spine.set_visible(False) for spine in axs[1, i].spines.values()]  # Hide all spines\n",
    "\n",
    "    #         # Add ylabel for each row (only set ylabel for the first column of each row)\n",
    "    #         axs[0, 0].set_ylabel(\"Original Images\", fontsize=12, labelpad=10)  # Y-axis label for the first row\n",
    "    #         axs[1, 0].set_ylabel(\"Augmented Images\", fontsize=12, labelpad=10)  # Y-axis label for the second row\n",
    "\n",
    "    #     # Adjust layout to make sure images and titles don't overlap\n",
    "    #     plt.tight_layout()\n",
    "\n",
    "    #     # Auto-scale to fit the images in the figure area\n",
    "    #     plt.autoscale(enable=True, axis='both', tight=True)\n",
    "    #     plt.show()\n",
    "        \n",
    "# from skimage import transform, color  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
