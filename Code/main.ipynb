{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Welcome to Medical image processing in Python***<br/>\n",
    "\n",
    "Presented by: Reza Saadatyar (2024-2025) <br/>\n",
    "E-mail: Reza.Saadatyar@outlook.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1️⃣ Set Image Path & Load data**<br/>\n",
    "\n",
    "**2️⃣ Convert the images into an array & masks to boolean**<br/>\n",
    "\n",
    "**3️⃣ RGB to gray**<br/>\n",
    "\n",
    "**4️⃣ [Image Resizing](https://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize)**<br/>\n",
    "`Standardizing Image Dimensions:` Machine learning models, like CNNs, need input data with fixed dimensions. For instance, if a model requires images of size 224x224x3, all input images must be resized to that shape.<br/>\n",
    "`Reducing Computational Load:`Resizing images to smaller dimensions lowers computational costs, particularly with large datasets, and aids in faster training or inference for deep learning models.\n",
    "\n",
    "**5️⃣ Augmentation**<br/>\n",
    "*Purpose:*<br/>\n",
    "- `Increase Dataset Size:` Augmentation creates new training samples from existing ones, effectively increasing the dataset size.<br/>\n",
    "- `Improve Model Robustness:` Introducing variations such as rotations, flips, and zooms helps the model adapt more effectively to real-world scenarios.<br/>\n",
    "- `Prevent Overfitting:` Augmentation enhances variability, minimizing the likelihood of the model overfitting to the training data.<br/>\n",
    "\n",
    "*Augmentation Techniques:*<br/>\n",
    "- `Rotation:` Rotating images by a specified degree range (e.g., rotation_range=30).<br/>\n",
    "- `Flip:` Flipping images horizontally or vertically.<br/>\n",
    "- `Zoom:` Zooming in or out of images.<br/>\n",
    "- `Shift:` Translating images horizontally or vertically.<br/>\n",
    "- `Brightness/Contrast Adjustment:` Changing the brightness or contrast of images.<br/>\n",
    "- `Noise Addition:` Adding random noise to images.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Image-Width: n*<br/>\n",
    "*Image-Height: m*<br/>\n",
    "*Channels: c*<br/>\n",
    "*Planes: p*<br/>\n",
    "*Grayscale: (p, m, n)*<br/>\n",
    "*RGB: (p, m, n, c)*<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF000e' size=\"4.8\" face=\"Arial\"><b>Import modules</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore\n",
    "# from skimage.viewer import ImageViewer\n",
    "\n",
    "from Functions.filepath_extractor import FilePathExtractor\n",
    "from Functions.image_processor import ImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#070bee size=\"4.5\" face=\"Arial\"><b>1️⃣ Set Image Path & Load data</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mfile_names = ['ytma12_010804_benign2_ccd.tif', 'ytma12_010804_benign3_ccd.tif', 'ytma12_010804_malignant1_ccd.tif', 'ytma12_010804_malignant2_ccd.tif', 'ytma12_010804_malignant3_ccd.tif', 'ytma23_022103_benign1_ccd.tif', 'ytma23_022103_benign2_ccd.tif', 'ytma23_022103_benign3_ccd.tif', 'ytma23_022103_malignant1_ccd.tif', 'ytma23_022103_malignant2_ccd.tif', 'ytma23_022103_malignant3_ccd.tif', 'ytma49_042003_benign1_ccd.tif', 'ytma49_042003_benign2_ccd.tif', 'ytma49_042003_benign3_ccd.tif', 'ytma49_042003_malignant1_ccd.tif', 'ytma49_042003_malignant2_ccd.tif', 'ytma49_042003_malignant3_ccd.tif', 'ytma49_042203_benign1_ccd.tif', 'ytma49_042203_benign2_ccd.tif', 'ytma49_042203_benign3_ccd.tif', 'ytma49_042203_malignant1_ccd.tif', 'ytma49_042203_malignant2_ccd.tif', 'ytma49_042203_malignant3_ccd.tif', 'ytma49_042403_benign1_ccd.tif', 'ytma49_042403_benign2_ccd.tif', 'ytma49_042403_benign3_ccd.tif', 'ytma49_042403_malignant1_ccd.tif', 'ytma49_042403_malignant2_ccd.tif', 'ytma49_042403_malignant3_ccd.tif', 'ytma49_072303_benign1_ccd.tif', 'ytma49_072303_benign2_ccd.tif', 'ytma49_072303_malignant1_ccd.tif', 'ytma49_072303_malignant2_ccd.tif', 'ytma49_111003_benign1_ccd.tif', 'ytma49_111003_benign2_ccd.tif', 'ytma49_111003_benign3_ccd.tif', 'ytma49_111003_malignant1_ccd.tif', 'ytma49_111003_malignant2_ccd.tif', 'ytma49_111003_malignant3_ccd.tif', 'ytma49_111303_benign1_ccd.tif', 'ytma49_111303_benign2_ccd.tif', 'ytma49_111303_benign3_ccd.tif', 'ytma49_111303_malignant1_ccd.tif', 'ytma49_111303_malignant2_ccd.tif', 'ytma49_111303_malignant3_ccd.tif', 'ytma55_030603_benign1_ccd.tif', 'ytma55_030603_benign2_ccd.tif', 'ytma55_030603_benign3_ccd.tif', 'ytma55_030603_benign4_ccd.tif', 'ytma55_030603_benign5_ccd.tif', 'ytma55_030603_benign6_ccd.tif', 'ytma10_010704_benign1_ccd.tif', 'ytma10_010704_benign2_ccd.tif', 'ytma12_010804_benign1_ccd.tif', 'ytma10_010704_malignant2_ccd.tif', 'ytma10_010704_malignant3_ccd.tif', 'ytma10_010704_benign3_ccd.tif', 'ytma10_010704_malignant1_ccd.tif']\n",
      "\u001b[34mfolder = ['D:/Medical-Image-Processing/Data/Inputs', 'D:/Medical-Image-Processing/Data/Inputs/A', 'D:/Medical-Image-Processing/Data/Inputs/A/B', 'D:/Medical-Image-Processing/Data/Inputs/A/B/C']\n",
      "\u001b[35mfiles_path = ['D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma12_010804_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma23_022103_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042003_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042203_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_042403_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_072303_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111003_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_malignant1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma49_111303_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign4_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign5_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/ytma55_030603_benign6_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/ytma10_010704_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/ytma10_010704_benign2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/ytma12_010804_benign1_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/ytma10_010704_malignant2_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/ytma10_010704_malignant3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/C/ytma10_010704_benign3_ccd.tif', 'D:/Medical-Image-Processing/Data/Inputs/A/B/C/ytma10_010704_malignant1_ccd.tif']\n",
      "\u001b[36msubfoldersname = ['A', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Inputs\"\n",
    "\n",
    "# Create an instance of DirectoryReader with the directory path and file format\n",
    "obj_path = FilePathExtractor(directory_path, format_type=\"tif\")\n",
    "file_names = obj_path.filesname          # List of filesname in the directory with the specified extension\n",
    "folder = obj_path.folders_path           # List of folders path where the files are located\n",
    "files_path = obj_path.all_files_path     # List of full files path for the files\n",
    "subfoldersname = obj_path.subfoldersname # List of subfolders name within the directory\n",
    "\n",
    "print(Fore.GREEN + f\"{file_names = }\"\"\\n\" + Fore.BLUE + f\"{folder = }\"\"\\n\" + Fore.MAGENTA + f\"{files_path = }\"+\n",
    "      \"\\n\" + Fore.CYAN + f\"{subfoldersname = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#09eb14 size=\"4.5\" face=\"Arial\"><b> 2️⃣ Convert the images into an array</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdata.shape = (58, 768, 896, 3)\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Inputs\"\n",
    "obj = ImageProcessor()\n",
    "data = obj.imgs_to_ndarray(directory_path, format_type=\"tif\")\n",
    "print(Fore.GREEN + f\"{data.shape = }\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#09eb14 size=\"4.5\" face=\"Arial\"><b> 2️⃣ Masks to boolean</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mmasks.shape = (58, 768, 896, 2)\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Masks/\"\n",
    "masks = obj.masks_to_boolean(directory_path, format_type=\"TIF\")\n",
    "print(Fore.GREEN + f\"{masks.shape = }\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#dd7f05 size=\"4.5\" face=\"Arial\"><b>3️⃣ RGB to Gray</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThe images have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"D:/Medical-Image-Processing/Data/Inputs/\"\n",
    "save_path = 'D:/Medical-Image-Processing/Data/'\n",
    "img_gray = obj.rgb_to_gray(directory_path, save_path, format_type=\"tif\", save_img_gray=\"on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#ec0995 size=\"4.5\" face=\"Arial\"><b>4️⃣ Image Resizing</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mResizing images from (58, 768, 896, 3) to (58, 255, 255, 3)\n"
     ]
    }
   ],
   "source": [
    "# Call the `resize_images` method to resize the images to the target dimensions (255x255)\n",
    "resized_images = obj.resize_images(data, img_height_resized=255, img_width_resized=255)  # Resize all images to 255x255\n",
    "print(Fore.GREEN + f\"Resizing images from {data.shape} to {resized_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#06defa size=\"4.5\" face=\"Arial\"><b>5️⃣ Augmentation</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "rotation_range = 30\n",
    "num_augmented_imag = 3\n",
    "# Path to the folder containing the original images\n",
    "file_path = 'D:/Medical-Image-Processing/Data/Inputs/'\n",
    "\n",
    "# Path where augmented images will be saved\n",
    "augmente_path = 'D:/Medical-Image-Processing/Data/'\n",
    "\n",
    "obj.augmentation(file_path, augmente_path, num_augmented_imag, rotation_range, format_type=\"tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "\n",
    "TRAIN_IMAGE_PATH = 'D:/Medical-Image-Processing/Inputs_Train'\n",
    "TRAIN_MASK_PATH = 'D:/Medical-Image-Processing/Masks_Train/'\n",
    "TEST_IMAGE_PATH = 'D:/Medical-Image-Processing/Inputs_Test/'\n",
    "TEST_MASK_PATH = 'D:/Medical-Image-Processing/Masks_Test/'\n",
    "\n",
    "Train_Mask_List = sorted(next(os.walk(TRAIN_MASK_PATH))[2])\n",
    "Test_Mask_List = sorted(next(os.walk(TEST_MASK_PATH))[2])\n",
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 256, 256, 3\n",
    "Init_Image = np.zeros((len(Train_Mask_List), 768, 896, 3), dtype = np.uint8)\n",
    "Init_Mask = np.zeros((len(Train_Mask_List), 768, 896), dtype = bool)\n",
    "Train_X = np.zeros((len(Train_Mask_List), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype = np.uint8)\n",
    "Train_Y = np.zeros((len(Train_Mask_List), IMG_HEIGHT, IMG_WIDTH, 1), dtype = bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "for mask_path in glob.glob('{}/*.TIF'.format(TRAIN_MASK_PATH)):\n",
    "    \n",
    "    base = os.path.basename(mask_path)\n",
    "    image_ID, ext = os.path.splitext(base)\n",
    "    image_path = '{}/{}_ccd.tif'.format(TRAIN_IMAGE_PATH, image_ID)\n",
    "    mask = io.imread(mask_path)\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    y_coord, x_coord = np.where(mask == 255)\n",
    "    \n",
    "    y_min = min(y_coord) \n",
    "    y_max = max(y_coord)\n",
    "    x_min = min(x_coord)\n",
    "    x_max = max(x_coord)\n",
    "    \n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "    cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    Train_X[n] = transform.resize(cropped_image[:,:,:IMG_CHANNELS],\n",
    "            (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True)\n",
    "    \n",
    "    Train_Y[n] = np.expand_dims(transform.resize(cropped_mask, \n",
    "            (IMG_HEIGHT, IMG_WIDTH),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True), axis = -1)\n",
    "    \n",
    "    Init_Image[n] = image\n",
    "    Init_Mask[n] = mask\n",
    "    \n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_X = np.zeros((len(Test_Mask_List), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype = np.uint8)\n",
    "Test_Y = np.zeros((len(Test_Mask_List), IMG_HEIGHT, IMG_WIDTH, 1), dtype = bool)\n",
    "    \n",
    "n = 0\n",
    "\n",
    "for mask_path in glob.glob('{}/*.TIF'.format(TEST_MASK_PATH)):\n",
    "    \n",
    "    base = os.path.basename(mask_path)\n",
    "    image_ID, ext = os.path.splitext(base)\n",
    "    image_path = '{}/{}_ccd.tif'.format(TEST_IMAGE_PATH, image_ID)\n",
    "    mask = io.imread(mask_path)\n",
    "    image = io.imread(image_path)\n",
    "    \n",
    "    y_coord, x_coord = np.where(mask == 255)\n",
    "    \n",
    "    y_min = min(y_coord) \n",
    "    y_max = max(y_coord)\n",
    "    x_min = min(x_coord)\n",
    "    x_max = max(x_coord)\n",
    "    \n",
    "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "    cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    Test_X[n] = transform.resize(cropped_image[:,:,:IMG_CHANNELS],\n",
    "            (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True)\n",
    "    \n",
    "    Test_Y[n] = np.expand_dims(transform.resize(cropped_mask, \n",
    "            (IMG_HEIGHT, IMG_WIDTH),\n",
    "            mode = 'constant',\n",
    "            anti_aliasing=True,\n",
    "            preserve_range=True), axis = -1)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rows = 1\n",
    "columns = 4\n",
    "Figure = plt.figure(figsize=(15,15))\n",
    "Image_List = [Init_Image[0], Init_Mask[0], Train_X[0], Train_Y[0]]\n",
    "p= ['Original_Image', 'Original_Mask', 'Region_of_Interest_Image', 'Region_of_Interest_Mask']\n",
    "n=0\n",
    "for i in range(1, rows*columns + 1):\n",
    "    Image = Image_List[i-1]\n",
    "    Sub_Plot_Image = Figure.add_subplot(rows, columns, i)\n",
    "    Sub_Plot_Image.imshow(np.squeeze(Image))\n",
    "    plt.title(p[n])\n",
    "    n +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero Padding\n",
    "  - `Preserve Dimensions:` Zero padding retains the original dimensions of the input tensor during convolution, which is important for tasks like image segmentation.\n",
    "  - `Control Output Size:` Zero padding ensures the output tensor size matches desired dimensions, especially in U-Net architectures where input and output sizes must align.\n",
    "  - `Avoid Information Loss:` Pixels near the edges are involved in fewer convolutions than central pixels, leading to potential information loss. Padding ensures every pixel is treated equally.\n",
    "  - `Better Performance:` By maintaining the spatial dimensions, zero padding helps the network extract features effectively, especially for deeper layers.\n",
    "  - Types of Padding\n",
    "    - `Valid Padding:` No padding is applied; the output size decreases.\n",
    "    - `Same Padding:` Zero padding is applied to maintain the same dimensions between input and output.\n",
    "- `Filter`: A filter (or kernel) is a small matrix that slides over input data to perform convolution, extracting features like edges, textures, and patterns (e.g., A 3x3 filter would have 9 weights).\n",
    "- `Stride` is the step size by which the filter or kernel moves across the input during convolution or pooling operations.\n",
    "  - *A stride of 1* moves the filter by one pixel at a time, resulting in a larger output.\n",
    "  - *A stride of 2* skips every alternate pixel, reducing the output size.\n",
    "  - $\\text{Output Size} =  \\large \\frac{\\text{Input Size} - \\text{Filter Size} + 2 \\times \\text{Padding}}{\\text{Stride}} + 1$\n",
    "    - Input Size (L) = 32\n",
    "    - Filter Size (K) = 3\n",
    "    - Padding (P), (P=0,valid padding)\n",
    "    - Stride (S) = 1\n",
    "    - $\\text{Output Size} = \\frac{32 - 3 + 2 \\times 1}{1} + 1 = \\frac{30 + 2}{1} + 1 = 32$\n",
    "- `Pooling` reduces the spatial dimensions of the feature map, lowering computational complexity and capturing dominant features.\n",
    "   - Max Pooling: 2×2 window, [1, 3; 2, 4] → 4.\n",
    "   - Average Pooling: 2×2 window, [1, 3; 2, 4] → 2.5.\n",
    "- `Upsampling` increases feature map dimensions, used in image segmentation and super-resolution. Methods include Nearest Neighbor Upsampling, which fills new positions with the nearest pixel value, and Bilinear Interpolation, which uses weighted averages for smoother outputs.\n",
    "- `Unpooling` Unpooling reverses pooling to restore feature map resolution, used in image reconstruction or segmentation. Max unpooling replaces max values from pooling in original positions, filling others with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Implementation of U_NET Model for Semantic Segmentation\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "def U_Net_Segmentation(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
    "    \n",
    "    \n",
    "    inputs = layers.Input(input_size)\n",
    "    n = layers.Lambda(lambda x:x/255)(inputs)\n",
    "    \n",
    "    c1 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(n)\n",
    "    c1 = layers.Dropout(0.1)(c1)\n",
    "    c1 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2,2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p1)\n",
    "    c2 = layers.Dropout(0.1)(c2)\n",
    "    c2 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2,2))(c2)\n",
    "\n",
    "\n",
    "    c3 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p2)\n",
    "    c3 = layers.Dropout(0.2)(c3)\n",
    "    c3 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2,2))(c3)\n",
    "\n",
    "\n",
    "    c4 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p3)\n",
    "    c4 = layers.Dropout(0.2)(c4)\n",
    "    c4 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2,2))(c4)\n",
    "\n",
    "\n",
    "    c5 = layers.Conv2D(256, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p4)\n",
    "    c5 = layers.Dropout(0.3)(c5)\n",
    "    c5 = layers.Conv2D(256, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c5)\n",
    "\n",
    "\n",
    "    u6 = layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u6)\n",
    "    c6 = layers.Dropout(0.2)(c6)\n",
    "    c6 = layers.Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c6)   \n",
    "\n",
    "    u7 = layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u7)\n",
    "    c7 = layers.Dropout(0.2)(c7)\n",
    "    c7 = layers.Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c7) \n",
    "\n",
    "    u8 = layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u8)\n",
    "    c8 = layers.Dropout(0.1)(c8)\n",
    "    c8 = layers.Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c8) \n",
    "    \n",
    "    \n",
    "    u9 = layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1], axis = 3)\n",
    "    c9 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u9)\n",
    "    c9 = layers.Dropout(0.1)(c9)\n",
    "    c9 = layers.Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c9) \n",
    "    \n",
    "    outputs = layers.Conv2D(1,(1,1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=[Mean_IOU_Evaluator])\n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# 6. Define U_NET Model Evaluator (Intersection Over Union _ IOU)\n",
    "# def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "#     prec = []\n",
    "\n",
    "#     # Loop over thresholds from 0.5 to 0.95 (step of 0.05)\n",
    "#     for t in np.arange(0.5, 1, 0.05):\n",
    "#         # Update deprecated functions\n",
    "#         y_pred_ = tf.cast(y_pred > t, tf.int32)  # Replace tf.to_int32 with tf.cast\n",
    "#         metric = tf.keras.metrics.MeanIoU(num_classes=2)  # Updated MeanIoU usage\n",
    "#         metric.update_state(y_true, y_pred_)\n",
    "#         prec.append(metric.result())  # Append IOU result for the threshold\n",
    "\n",
    "#     # Return the mean IOU across all thresholds\n",
    "#     return K.mean(K.stack(prec), axis=0)\n",
    "model = U_Net_Segmentation()\n",
    "\n",
    "def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "    prec = []\n",
    "    metric = tf.keras.metrics.MeanIoU(num_classes=2)  # Create the metric instance outside the loop\n",
    "\n",
    "    # Loop over thresholds from 0.5 to 0.95 (step of 0.05)\n",
    "    for t in np.arange(0.5, 1, 0.05):\n",
    "        y_pred_ = tf.cast(y_pred > t, tf.int32)  # Apply threshold\n",
    "        \n",
    "        # Reset the internal variables of MeanIoU\n",
    "        metric.update_state([], [])  # Workaround to clear previous state\n",
    "\n",
    "        metric.update_state(y_true, y_pred_)\n",
    "        prec.append(metric.result().numpy())  # Append IOU result for the threshold\n",
    "\n",
    "    # Return the mean IOU across all thresholds\n",
    "    return K.mean(K.stack(prec), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. Define U_NET Model Evaluator (Intersection Over Union _ IOU)\n",
    "# from tensorflow.keras import backend as k\n",
    "# def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "    \n",
    "#     prec = []\n",
    "    \n",
    "#     for t in np.arange(0.5, 1, 0.05):\n",
    "        \n",
    "#         # y_pred_ = tf.to_int32(y_pred>t)\n",
    "#         y_pred_ = tf.cast(y_pred > t, tf.int32)  # Replace tf.to_int32 with tf.cast\n",
    "#         score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "#         k.get_session().run(tf.local_variables_initializer())\n",
    "#         with tf.control_dependencies([up_opt]):\n",
    "#             score = tf.identity(score)\n",
    "#         prec.append(score)\n",
    "#     return k.mean(k.stack(prec), axis = 0)\n",
    "\n",
    "# model = U_Net_Segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageset = 'BCC'\n",
    "backbone = 'UNET'\n",
    "version = 'v1.0'\n",
    "\n",
    "# Correct file path to end with .keras\n",
    "model_keras = 'model-{imageset}-{backbone}-{version}.keras'.format(\n",
    "    imageset=imageset, backbone=backbone, version=version\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "earlystopper = callbacks.EarlyStopping(patience=7, verbose=1)\n",
    "checkpointer = callbacks.ModelCheckpoint(\n",
    "    filepath=model_keras,  # Use the correct file path ending with .keras\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Show The Results per Epoch\n",
    "\n",
    "class loss_history(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__ (self, x=4):\n",
    "        self.x = x\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        \n",
    "        io.imshow(Train_X[self.x])\n",
    "        plt.show()\n",
    "        \n",
    "        io.imshow(np.squeeze(Train_Y[self.x]))\n",
    "        plt.show()\n",
    "        \n",
    "        preds_train = self.model.predict(np.expand_dims(Train_X[self.x], axis = 0))\n",
    "        io.imshow(np.squeeze(preds_train[0]))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(Train_X, Train_Y,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=2,\n",
    "                    epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Show The Results per Epoch\n",
    "\n",
    "class loss_history(callbacks.Callback):\n",
    "\n",
    "    def __init__ (self, x=4):\n",
    "        self.x = x\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \n",
    "        io.imshow(Train_X[self.x])\n",
    "        plt.show()\n",
    "        \n",
    "        io.imshow(np.squeeze(Train_Y[self.x]))\n",
    "        plt.show()\n",
    "        \n",
    "        preds_train = self.model.predict(np.expand_dims(Train_X[self.x], axis = 0))\n",
    "        io.imshow(np.squeeze(preds_train[0]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train U_NET Model using Training Samples\n",
    "\n",
    "results = model.fit(Train_X, Train_Y, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size=2,\n",
    "                    epochs=20,\n",
    "                    callbacks=[earlystopper, checkpointer, loss_history()]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_img_original_augment(self, num_img: int) -> None:\n",
    "        \n",
    "#         _, axs = plt.subplots(nrows=2, ncols=num_img)\n",
    "#          # Check if num_img is 1 (special case for 1 image)\n",
    "#         if num_img == 1:\n",
    "#             # Display images on the first row\n",
    "#             # io.imread(files_inputs[0])[:, :, :3].shape\n",
    "#             axs[0].imshow(io.imread(glob.glob(self.file_path + '/*')[0]), cmap='gray')\n",
    "#             axs[0].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "#             [spine.set_visible(False) for spine in axs[0].spines.values()]  # Hide all spines\n",
    "#             axs[0].set_ylabel(\"Original Images\", fontsize=12, labelpad=10)  # Y-axis label for the first row\n",
    "            \n",
    "#             # Display images on the second row\n",
    "#             axs[1].imshow(io.imread(glob.glob(self.ugmente_path + '/*')[0]), cmap='gray')\n",
    "#             axs[1].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "#             [spine.set_visible(False) for spine in axs[1].spines.values()]  # Hide all spines\n",
    "            # axs[1].set_ylabel(\"Augmented Images\", fontsize=12, labelpad=10)  # Y-axis label for the second row\n",
    "\n",
    "    #     else:\n",
    "    #         for i in range(num_img):\n",
    "    #             # Display images on the first row\n",
    "    #             axs[0, i].imshow(io.imread(glob.glob(self.imag_files_path + '/*')[i]), cmap='gray')\n",
    "    #             axs[0, i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "    #             [spine.set_visible(False) for spine in axs[0, i].spines.values()]  # Hide all spines\n",
    "\n",
    "    #             # Display images on the second row\n",
    "    #             axs[1, i].imshow(io.imread(glob.glob(self.imag_augmented_path + '/*')[i]), cmap='gray')\n",
    "    #             axs[1, i].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)  # Hide ticks\n",
    "    #             [spine.set_visible(False) for spine in axs[1, i].spines.values()]  # Hide all spines\n",
    "\n",
    "    #         # Add ylabel for each row (only set ylabel for the first column of each row)\n",
    "    #         axs[0, 0].set_ylabel(\"Original Images\", fontsize=12, labelpad=10)  # Y-axis label for the first row\n",
    "    #         axs[1, 0].set_ylabel(\"Augmented Images\", fontsize=12, labelpad=10)  # Y-axis label for the second row\n",
    "\n",
    "    #     # Adjust layout to make sure images and titles don't overlap\n",
    "    #     plt.tight_layout()\n",
    "\n",
    "    #     # Auto-scale to fit the images in the figure area\n",
    "    #     plt.autoscale(enable=True, axis='both', tight=True)\n",
    "    #     plt.show()\n",
    "        \n",
    "# from skimage import transform, color  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
