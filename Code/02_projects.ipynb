{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHyGmngPOYww"
      },
      "source": [
        "***Welcome to Medical image processing in Python***<br/>\n",
        "\n",
        "Presented by: Reza Saadatyar (2024-2025) <br/>\n",
        "E-mail: Reza.Saadatyar@outlook.com<br/>\n",
        "**[GitHub](https://github.com/RezaSaadatyar/Deep-Learning-in-python)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNp-9_DrOYwz"
      },
      "source": [
        "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597)\n",
        "\n",
        "**Outline:**<br/>\n",
        "**▪ Importing the dataset**<br/>\n",
        "**▪ Convert the images into array & masks to binary**<br/>\n",
        "**▪ [Prepare TensorFlow datasets for training, validation, and testing](https://github.com/RezaSaadatyar/Deep-Learning-in-python/blob/main/Code/02_tensorflow_data_loading_and_processing.ipynb)**<br/>\n",
        "**▪ U-Net Architectur**<br/>\n",
        "**▪ Segmentation Metrics**<br/>\n",
        "**▪ Callbacks**<br/>\n",
        "**▪ Training the model**<br/>\n",
        "**▪ Test the model**<br/>\n",
        "**▪ Load previously trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This code is specifically designed for Google Colab environment\n",
        "# It clones a GitHub repository containing medical image processing code and sets up the environment\n",
        "\n",
        "# Clone the Medical-Image-Processing repository from GitHub using git command\n",
        "# In Colab, this will clone into the /content directory by default\n",
        "!git clone https://github.com/RezaSaadatyar/Medical-Image-Processing\n",
        "\n",
        "# List contents of the cloned repository directory to verify successful cloning\n",
        "# Colab's !ls command shows files in the current directory\n",
        "!ls /content/Medical-Image-Processing\n",
        "\n",
        "# Change working directory to the Code subdirectory of the cloned repository\n",
        "# %cd is a Colab magic command to change directories\n",
        "%cd /content/Medical-Image-Processing/Code\n",
        "\n",
        "# Import Python's system module for path manipulation\n",
        "import sys\n",
        "\n",
        "# Add the Functions subdirectory to Python's system path for module imports\n",
        "# This allows importing custom modules from the cloned repository\n",
        "sys.path.append('/content/Medical-Image-Processing/Code/Functions')\n",
        "\n",
        "# Install the colorama package using pip for colored terminal text\n",
        "# Colab's !pip install installs packages in the current runtime environment\n",
        "! pip install colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFmRYLHkOYw1"
      },
      "source": [
        "<font color='#FF000e' size=\"4.8\" face=\"Arial\"><b>Importing libraries</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4N3a0MATOYw2"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Union\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from Functions.image_processor import ImageProcessor\n",
        "from Functions.filepath_extractor import FilePathExtractor\n",
        "from Functions.prepare_dataset import prepare_dataset\n",
        "from Functions.unet_model import unet_model\n",
        "from Functions.segmentation_metrics import segmentation_metrics\n",
        "from Functions.show_image_training_progress import ShowImageTrainingProgress\n",
        "\n",
        "from Functions.plot_training_history import plot_training_history\n",
        "from Functions.plot_evaluation_results import plot_evaluation_results\n",
        "from Functions.evaluate_segmentation_predictions import evaluate_segmentation_predictions\n",
        "from Functions.display_predictions import display_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoAsGhrxOYw4"
      },
      "source": [
        "<font color=#eff30b size=\"4.5\" face=\"Arial\"><b>1️⃣ Project 1: Breast Cancer Image Segmentation</b></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BrLbB2FOYw4",
        "outputId": "a189c4bd-3f51-4a1c-d5eb-1de410803de9"
      },
      "outputs": [],
      "source": [
        "# ======================================= 1. Importing the dataset =============================================\n",
        "# ------------------------------------------------ Images ------------------------------------------------------\n",
        "img_format_type = \"png\"   # png, jpg, jpeg, tiff, bmp, tif, etc.\n",
        "image_path = \"D:/Medical-Image-Processing/Data/Breast cancer/images/\"\n",
        "# image_path = \"/content/Medical-Image-Processing/Data/Membrance/images\"\n",
        "\n",
        "# Create an instance of DirectoryReader with the directory path and file format\n",
        "obj_inputs = FilePathExtractor(directory_path=image_path, format_type=img_format_type)\n",
        "img_filesname = obj_inputs.filesname         # List of filesname in the directory\n",
        "\n",
        "# ------------------------------------------------ Masks -------------------------------------------------------\n",
        "mask_format_type = \"png\"  # png, jpg, jpeg, tiff, bmp, tif, etc.\n",
        "\n",
        "# mask_path = \"D:/Medical-Image-Processing/Data/Membrance/masks/\"\n",
        "mask_path = \"/content/Medical-Image-Processing/Data/Membrance/masks\"\n",
        "\n",
        "# Create an instance of DirectoryReader with the directory path and file format\n",
        "obj_masks = FilePathExtractor(directory_path=mask_path, format_type=mask_format_type)\n",
        "mask_filesname = obj_masks.filesname      # List of filesname in the directory\n",
        "\n",
        "# -------------------------------------------- Check of the Images ---------------------------------------------\n",
        "# Compare the base filenames (without extensions) of input files and mask files\n",
        "# This line checks if the list of base filenames for input files matches the list of base filenames for mask files\n",
        "[i.split(img_format_type)[0] for i in img_filesname] == [i.split(mask_format_type)[0] for i in mask_filesname]\n",
        "\n",
        "# ============================ 2.Convert the images into array & masks to binary ===============================\n",
        "# Create an instance of the `ImageProcessor` class\n",
        "obj = ImageProcessor()\n",
        "\n",
        "# Use the `read_images` method of the `ImageProcessor` object to load images from the directory\n",
        "# The images are expected to be in \"png or tif\" format, and they are converted into a NumPy array\n",
        "images = obj.read_images(image_path=image_path, format_type=img_format_type, resize=None, normalize=True)\n",
        "\n",
        "# Use the `mask_read` method of the `ImageProcessor` object to load mask images from the directory\n",
        "# The masks are expected to be in \"TIF\" format, and they are converted into a NumPy array\n",
        "masks = obj.read_masks(mask_path=mask_path, format_type=mask_format_type, resize=None, normalize=True, num_classes=2)\n",
        "\n",
        "ind = 20 # Set the index of the image and mask to display\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.subplot(121); plt.imshow(images[ind], cmap='gray'); plt.title(f'Image {ind}', fontsize=10)\n",
        "plt.subplot(122); plt.imshow(masks[ind], cmap='gray'); plt.title(f'Mask {ind}', fontsize=10)\n",
        "plt.tight_layout() # Adjust the layout to prevent overlap between subplots\n",
        "\n",
        "# ===================== Prepare TensorFlow datasets for training, validation, and testing ======================\n",
        "batch_size = 4\n",
        "\n",
        "# Assuming data_resize and masks_resize are your input data and masks\n",
        "train_dataset, valid_dataset, test_dataset = prepare_dataset(\n",
        "    data=images,\n",
        "    labels=masks,\n",
        "    train_size=0.6,\n",
        "    valid_size=0.25,\n",
        "    batch_size=batch_size,\n",
        "    shuffle_train=True,\n",
        "    shuffle_buffer_size=1000,\n",
        ")\n",
        "\n",
        "# ================================================= U-Net model ================================================\n",
        "# Convert any tf.placeholder usage to tf.compat.v1.placeholder\n",
        "tf.placeholder = tf.compat.v1.placeholder if hasattr(tf, 'placeholder') else None\n",
        "img_height, img_width, img_channels = list(train_dataset.element_spec[0].shape[1:])\n",
        "model = unet_model(img_height,\n",
        "                   img_width,\n",
        "                   img_channels,\n",
        "                   base_filters=16,\n",
        "                   kernel_size=(3, 3),\n",
        "                   transpose_kernel_size=(2, 2),\n",
        "                   act_dropout=False,\n",
        "                   use_batchnorm=False,\n",
        "                   dropout_rates={'shallow':0.1, 'mid':0.2, 'deep':0.3}  # Custom rates\n",
        "                  )\n",
        "# model.summary()\n",
        "\n",
        "# ======================================= Segmentation Metrics & compile =======================================\n",
        "metrics = segmentation_metrics(alpha=0.1)\n",
        "\n",
        "adam = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)\n",
        "\n",
        "# Compile the model with custom loss and metrics\n",
        "model.compile(\n",
        "    optimizer=adam,               # Or 'adam', 'rmsprop'\n",
        "    loss='binary_crossentropy', # Or binary_crossentropy, 'sparse_categorical_crossentropy', metrics['dice_coef_loss'], 'iou_coef_loss', 'combined_loss'\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        metrics['dice_coef'],\n",
        "        metrics['iou'],\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        ],\n",
        "    run_eagerly=False,  # Set run_eagerly=False for better performance\n",
        "    )\n",
        "\n",
        "# ================================================ Callbacks ===================================================\n",
        "# Define the callbacks\n",
        "# csv_path = os.path.join(\"files\", \"log.csv\")\n",
        "csv_path = 'training_log.csv'  # File to log training metrics\n",
        "model_checkpoint_path = 'model.keras'  # File to save the best model\n",
        "\n",
        "callbacks = [\n",
        "    CSVLogger(csv_path),  # Logs metrics to a CSV file\n",
        "    ShowImageTrainingProgress(train_dataset, sample_idx=0),  # Show the first sample in the batch\n",
        "    ModelCheckpoint(\n",
        "        filepath=model_checkpoint_path,\n",
        "        monitor='val_loss',   # Monitor validation loss\n",
        "        verbose=1,            # Print when saving\n",
        "        save_best_only=True   # Save only when val_loss improves\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',   # Monitor validation loss\n",
        "        factor=0.1,           # Reduce learning rate by 10x\n",
        "        patience=10,          # Wait 5 epochs without improvement\n",
        "        min_lr=1e-6,          # Minimum learning rate\n",
        "        verbose=1             # Print when reducing LR\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',         # Monitor validation loss\n",
        "        patience=10,                # Stop after 5 epochs without improvement\n",
        "        restore_best_weights=True   # Restore best weights (changed to True for better results)\n",
        "    )\n",
        "]\n",
        "\n",
        "# ============================================ Training the model ==============================================\n",
        "# Hyperparameters\n",
        "num_epochs = 50\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=valid_dataset,\n",
        "    shuffle=False,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "results = model.evaluate(test_dataset, return_dict=True, verbose=1)\n",
        "\n",
        "# ============================================ Test the model ==================================================\n",
        "predictions = model.predict(test_dataset, verbose=0)\n",
        "evaluate_segmentation_predictions(test_dataset=test_dataset, predictions=predictions, num_sample=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "oETktKJ4OYxF",
        "outputId": "180b8f93-355d-46f3-e2c5-cd8ca2f7a18f"
      },
      "outputs": [],
      "source": [
        "# Plot the history\n",
        "plot_training_history(history, figsize=(8, 4))\n",
        "plot_evaluation_results(results, figsize=(4, 4))\n",
        "\n",
        "# Extract a small batch of data from test_dataset for visualization\n",
        "for batch in test_dataset.take(1):\n",
        "    test_images, test_masks = batch\n",
        "    break\n",
        "\n",
        "# Convert to numpy arrays for easier manipulation\n",
        "test_images = test_images.numpy()\n",
        "test_masks = test_masks.numpy()\n",
        "display_predictions(test_images, test_masks, predictions, indx=0, figsize=(6, 4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
